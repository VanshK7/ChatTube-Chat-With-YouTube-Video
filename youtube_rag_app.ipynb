{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ0aP9okWNue"
      },
      "source": [
        "# **Chat with any YouTube video using RAG and Claude**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQUwM3RyWNuf",
        "outputId": "7a43888a-dcfb-4e2d-c17c-52296d61184c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "%env ANTHROPIC_API_KEY = YOUR_API_KEY\n",
        "\n",
        "CLAUDE_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uew0dghWNug",
        "outputId": "d95b2feb-e06c-49d7-eeda-381cef981972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.1/871.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing langchain anthropic (for Claude)\n",
        "!pip install langchain-anthropic -q\n",
        "!pip install langchain -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NlxZVHjEWNug"
      },
      "outputs": [],
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "model = ChatAnthropic(model='claude-3-haiku-20240307', )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mvf_poVYWNug",
        "outputId": "ef0c6070-08f5-4a4d-9f01-d98078070645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of Brazil is Brasília.', response_metadata={'id': 'msg_01GP4iVEzznK3epf5TfaTUae', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 14, 'output_tokens': 12}}, id='run-9779ae8f-c8c5-44af-a4d1-20d5e5c00275-0')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing the model\n",
        "model.invoke(\"What is the capital of Brazil?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WMoFQKnFWNug",
        "outputId": "5edf2155-809a-44b9-f176-d27a10aa6cce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Brasília.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To get only the string as output instead of the whole AIMessage...\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Defining the string parser\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Defining the chain\n",
        "chain = model | parser              # Output of model is passed as input to parser (chaining functions)\n",
        "chain.invoke(\"What is the capital of Brazil? Answer in 1 word.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTUzFL0yWNug"
      },
      "source": [
        "### **Adding a prompt template to modify it's behaviour**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IAe7ZbVKWNuh"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context given below. Only answer the question if you're confident. If you do not know the answer, then reply with \"I'm sorry, I don't know the answer to this\". Your job is to act like a helpful humanoid assistant who provides short but crisp answers.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mkncu04pWNuh",
        "outputId": "ab7f9056-c85a-4b60-cb79-fb034329773a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Human: \\nAnswer the question based on the context given below. Only answer the question if you\\'re confident. If you do not know the answer, then reply with \"I\\'m sorry, I don\\'t know the answer to this\". Your job is to act like a helpful humanoid assistant who provides short but crisp answers. No need to mention \\'context-provided\\'\\n\\nContext: John\\'s favourite colour is red, Jogn married Jane, she hates red but loves yellow!\\n\\nQuestion: Who is Jane?\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)     # Adding the prompt from the template\n",
        "\n",
        "# Adding the context and the prompt\n",
        "prompt.format(\n",
        "    context = \"John's favourite colour is red, Jogn married Jane, she hates red but loves yellow!\",\n",
        "    question = \"Who is Jane?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1DGA0ZWAWNuh",
        "outputId": "366e8d17-84a0-427a-9b20-1148e44975b9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Jane is John's wife.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extending the chain\n",
        "chain = prompt | model | parser\n",
        "\n",
        "chain.invoke({\n",
        "    \"context\": \"John's favourite colour is red, John married Jane, she hates red but loves yellow!\",\n",
        "    \"question\": \"Who is Jane?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNFl93JoWNuh"
      },
      "source": [
        "### **Combining Multiple Chains**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gj2jKq5LWNuh"
      },
      "outputs": [],
      "source": [
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate {answer} to {language}, act like the translated output is the only output.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a7_U348nWNuh",
        "outputId": "aa89a2a3-c573-44a7-9a8f-4902df718aa8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Jane est la femme de John.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")}  | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"John's favourite colour is red, John married Jane, she hates red but loves yellow!\",\n",
        "        \"question\": \"Who is Jane?\",\n",
        "        \"language\": \"French\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBgtWOOXWNuh"
      },
      "source": [
        "## **Transcribing YouTube Videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "V3TPYbFUWNui"
      },
      "outputs": [],
      "source": [
        "!pip install pytube -q\n",
        "!pip install openai-whisper -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hS71A-tWNui",
        "outputId": "0ce9b867-967c-413a-e2ae-226d806ab04d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 128MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading audio...\n",
            "Audio downloaded to /content/downloaded_audio.mp4\n",
            "Transcribing audio...\n",
            "Transcription completed and saved to transcription.txt\n",
            "Temporary audio file removed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=P6FORpg0KVo\"\n",
        "\n",
        "# Check if the transcription file already exists\n",
        "if not os.path.exists(\"transcription.txt\"):\n",
        "    # Create a YouTube object\n",
        "    youtube = YouTube(YOUTUBE_VIDEO)\n",
        "\n",
        "    # Get the audio stream\n",
        "    audio = youtube.streams.filter(only_audio=True).first()\n",
        "\n",
        "    # Load the base Whisper model\n",
        "    whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "    # Define the path to download the audio file\n",
        "    current_folder = os.getcwd()\n",
        "    audio_file_path = os.path.join(current_folder, \"downloaded_audio.mp4\")\n",
        "\n",
        "    # Download the audio file\n",
        "    print(\"Downloading audio...\")\n",
        "    audio.download(output_path=current_folder, filename=\"downloaded_audio.mp4\")\n",
        "    print(f\"Audio downloaded to {audio_file_path}\")\n",
        "\n",
        "    # Transcribe the downloaded audio file\n",
        "    print(\"Transcribing audio...\")\n",
        "    transcription = whisper_model.transcribe(audio_file_path, fp16=False)[\"text\"].strip()\n",
        "\n",
        "    # Write the transcription to a text file\n",
        "    with open(\"transcription.txt\", \"w\") as file:\n",
        "        file.write(transcription)\n",
        "    print(\"Transcription completed and saved to transcription.txt\")\n",
        "\n",
        "    # Remove the downloaded audio file\n",
        "    os.remove(audio_file_path)\n",
        "    print(\"Temporary audio file removed\")\n",
        "else:\n",
        "    print(\"Transcription file already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrv02UkfbW6z"
      },
      "source": [
        "### **Using the Entire Transcript as Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpu6gukNbhEF"
      },
      "source": [
        "We need to split the transcription as the models cannot take the entire transcript as context directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YfrNEifCWNui"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()        # Contains the entire text transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0pvLN4Ibov9",
        "outputId": "6e3679aa-42f9-4e87-d3cc-48b0e66f7d4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"So, I'm from Guatemala. This is a public service announcement that is word Guatemala is. Also, that is not where they keep the prisoners that is called Guantanamo. Not the same place. So, Guatemala is right below Mexico and for the Americans in the audience and let the sink in because it really applies in most ways. For the Americans in the audience, you can think of it as Mexico's Mexico. Just like the US doesn't want illegal immigration from Mexico, Mexico doesn't want illegal immigration from Guatemala. It's a smaller country. It's a poorer country and well, what can I tell you? It has much better Mexican food. Guatemala is a very poor country and a lot of people talk about education as something that brings equality to different social classes. But I always saw it as the opposite, as something that brings inequality. Because what happens in practice is that people who have a lot of money and by themselves be really good education and therefore continue having a lot of money.\", metadata={'source': 'transcription.txt'}),\n",
              " Document(page_content=\"and therefore continue having a lot of money. Whereas people who don't have very much money, barely learn how to read and write and therefore never make a lot of money and this is especially true in poor countries. Now, I was fortunate that I received a rich person's education even though I didn't grow up rich and it's because I'm an only child and my mother, who was a single mother, spent all of her resources on my education and this allowed me to come to college to the US and eventually get a PhD in computer science. Now, because of all of this, about 10 years ago I decided I wanted to do something that would give equal access to education to everyone. Oh, by the way, this is what I want to talk to you about today, giving equal access to education to everyone. At the time, I was a professor of computer science at Carnegie Mellon University and I decided to work on this with my PhD students every. The way my brain works, all of education is just too general of a problem. So I\", metadata={'source': 'transcription.txt'})]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Splitting the transcript into chunks of 1000 with an overlap of 50\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "text_splitter.split_documents(text_documents)[:2]       # Checking the first 2 chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IiWiWLBclY7"
      },
      "source": [
        "## **Finding the relevant chunks with embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyqFcHdhcpES"
      },
      "source": [
        "But how do we find the right chunks of data that contains the relevant information when a user asks a question?\n",
        "\n",
        "We convert the chunks and the user question to embeddings & find the k-nearest chunk embeddings to the user question's embeddings. Nearby chunks have the highest similarity, which is why we use this approach to find the relevant context for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1ZzJSc89d7Y0"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_PMjIAzemXS",
        "outputId": "dacb7984-beb5-4447-858f-02d5437b0829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roRASlu0cDtn",
        "outputId": "eaf26539-36e2-4724-c16b-942b653e6549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results for query 'Where is the speaker from?':\n",
            "a mobile phone or a smartphone in particular. See, building schools all over the world is simple to expensive. On the other hand, most of the world's population already has access to a smartphone and the trend is that that fraction is only going to increase. So we decided at the time that we would make a way to learn foreign languages on a mobile phone that was accessible to everyone. And then we called it Duolingo. Now, in order to truly be accessible to everyone, rich and poor, Duolingo uses a freemium model to support itself. What that means is that you can learn as much as you want without ever having to pay. But if you don't pay, you may have to see an ad at the end of a lesson. Now, if you don't like ads, you can also pay to subscribe to turn off the ads. And it turns out that the vast majority of the revenue for Duolingo comes from people to pay to subscribe to turn off the ads. Now, who are these people who pay to subscribe to turn off the ads? Well, they're usually people,\n",
            "So, I'm from Guatemala. This is a public service announcement that is word Guatemala is. Also, that is not where they keep the prisoners that is called Guantanamo. Not the same place. So, Guatemala is right below Mexico and for the Americans in the audience and let the sink in because it really applies in most ways. For the Americans in the audience, you can think of it as Mexico's Mexico. Just like the US doesn't want illegal immigration from Mexico, Mexico doesn't want illegal immigration from Guatemala. It's a smaller country. It's a poorer country and well, what can I tell you? It has much better Mexican food. Guatemala is a very poor country and a lot of people talk about education as something that brings equality to different social classes. But I always saw it as the opposite, as something that brings inequality. Because what happens in practice is that people who have a lot of money and by themselves be really good education and therefore continue having a lot of money.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Read the text from a file\n",
        "with open(\"transcription.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Splitting the text into chunks of 1000 with an overlap of 50\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(text_documents)\n",
        "\n",
        "# Load the HuggingFace embeddings\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create a FAISS vector store from the text chunks\n",
        "vector_store = FAISS.from_texts(texts=text_splitter.split_text(text), embedding=hf_embeddings)\n",
        "\n",
        "# Now you can use the vector store for semantic search or other tasks\n",
        "query = \"Where is the speaker from?\"\n",
        "results = vector_store.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"Results for query '{query}':\")\n",
        "for result in results:\n",
        "    print(result.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgpEEl_NfJyy"
      },
      "source": [
        "See how the chunks related to Guatemala (the place the speaker is from) are retrieved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gos6Kmsafe4t"
      },
      "source": [
        "## **Connecting to Pinecone vector database (Optional)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSgOa48ufo9K",
        "outputId": "8591de9d-00e5-4404-a492-e6a32aaa0c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.11.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.0.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_pinecone -q\n",
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBgIhJKyjJfO",
        "outputId": "aa5e9372-09a1-44f9-c200-828f550db15f"
      },
      "outputs": [],
      "source": [
        "%env PINECONE_API_KEY = YOUR_API_KEY\n",
        "%env PINECONE_ENV = youtube_rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "QeSVslfFiTv9"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# Create an instance of the Pinecone class\n",
        "pinecone_client = pinecone.Pinecone(\n",
        "    api_key = os.getenv(\"PINECONE_API_KEY\"),\n",
        "    environment = os.getenv(\"PINECONE_ENV\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1uNsOD9Ed4bF"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "index_name = \"yt-rag\"\n",
        "\n",
        "# To store our chunks of data and the embeddings in Pinecone\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    documents,\n",
        "    hf_embeddings,\n",
        "    index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGXlgWT4fqUT",
        "outputId": "ad03caf9-e8b7-4ed2-e0f0-da76697ff4a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content=\"a mobile phone or a smartphone in particular. See, building schools all over the world is simple to expensive. On the other hand, most of the world's population already has access to a smartphone and the trend is that that fraction is only going to increase. So we decided at the time that we would make a way to learn foreign languages on a mobile phone that was accessible to everyone. And then we called it Duolingo. Now, in order to truly be accessible to everyone, rich and poor, Duolingo uses a freemium model to support itself. What that means is that you can learn as much as you want without ever having to pay. But if you don't pay, you may have to see an ad at the end of a lesson. Now, if you don't like ads, you can also pay to subscribe to turn off the ads. And it turns out that the vast majority of the revenue for Duolingo comes from people to pay to subscribe to turn off the ads. Now, who are these people who pay to subscribe to turn off the ads? Well, they're usually people,\", metadata={'source': 'transcription.txt'})]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's test it\n",
        "pinecone.similarity_search(\"Tell me about the ideology behind Duolingo\")[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD3ssV4CkcL9"
      },
      "source": [
        "## **Creating a chain with everything connected**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "n8w4YUJFkZ5J",
        "outputId": "6bb2f3a8-aceb-458c-f9ca-4240efcb6e0b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Based on the given context, the philosophy behind Duolingo seems to be making high-quality education accessible to everyone, both rich and poor, through the use of mobile phones and a freemium business model.\\n\\nThe key points are:\\n\\n1. Duolingo was created with the goal of making it possible to learn foreign languages on a mobile phone, as most of the world's population already has access to smartphones.\\n\\n2. Duolingo uses a freemium model, where users can learn for free but can optionally pay to remove ads. The majority of Duolingo's revenue comes from these paid subscriptions.\\n\\n3. The aim is to make education accessible to everyone, regardless of their financial status, by leveraging the widespread availability of smartphones.\""
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "\n",
        "chain.invoke(\"What is the philosophy behind Duolingo?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
